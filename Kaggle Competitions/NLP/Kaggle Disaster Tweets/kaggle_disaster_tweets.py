# -*- coding: utf-8 -*-
"""Kaggle Disaster Tweets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Yu1cbjM3PIciY8-rK-WPd8R7Xu1o_1th

# Libraries.
"""

# Commented out IPython magic to ensure Python compatibility.
## Libraries.
import csv # Writing to csv.
import pandas as pd # Data frames.
import numpy as np # Arrays.
import re # Regular expressions.

# Text Classification.
import torch
import torch.optim as optim
# !pip install torchtext==0.8.1
from torchtext import data
from torchtext.data import TabularDataset

import nltk # Natural Language Processing.
from nltk import word_tokenize
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download("stopwords") # Stopwords.

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
wnl = WordNetLemmatizer()
wst = nltk.tokenize.WhitespaceTokenizer()

from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
from sklearn.model_selection import train_test_split, KFold
from sklearn.feature_extraction.text import TfidfVectorizer

from wordcloud import WordCloud
import matplotlib.pyplot as plt
import seaborn as sns

# Mounting Google Drive onto Colab.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive

# Reproducibility.
SEED = 1234
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True

"""# Reading the data set."""

# Data sets.
training = pd.read_csv('/gdrive/MyDrive/Kaggle Disaster Tweets/train.csv')
test = pd.read_csv('/gdrive/MyDrive/Kaggle Disaster Tweets/test.csv')

# Stopwords.
en_stop = set(stopwords.words('english'))

# Checking the shape of the data frames.
training.shape, test.shape, len(en_stop)

"""# Exploring the data set."""

training.isna().sum()

test.isna().sum()

"""## Filtering out columns with missing data."""

training = training[['text', 'target']]
test = test[['id', 'text']]

"""# Filtering out the negatives."""

true_cases = training[training['target']==1]
text = ' '.join(word.split()[1] for word in true_cases.text)



"""## Generating the word cloud."""

word_cloud = WordCloud(collocations = False,
                       stopwords = en_stop,
                       background_color = 'black').generate(text)
plt.imshow(word_cloud, interpolation='bilinear')
plt.axis('off')
plt.show()

grp = round(training.groupby(['target'])['target'].value_counts(normalize=False)/training.shape[0]*100)
grp = grp.rename('percentage')
grp = grp.reset_index(0)
sns.barplot(x = grp.target, y = grp.percentage)

"""## Data Pre-Processing Class.

### Data Pre-Processing.
"""

test.isnull().sum()

"""# Data PreProcessing."""

import string
def cleaning(x):
  x = x.lower().strip()
  x = re.sub("[^:.//\s\w]|\n", ' ', x)
  
  x = [wnl.lemmatize(w) for w in wst.tokenize(x) if w not in en_stop]
  return ' '.join(x)

training['Preprocessed Text'] = training['text'].apply(lambda x: cleaning(x))
test['Preprocessed Text'] = test['text'].apply(lambda x: cleaning(x))

training

"""# Statistical Models."""

from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression

# Models.
SVC = LinearSVC()
MNB = MultinomialNB()
LR = LogisticRegression(solver = 'liblinear', penalty = 'l2')

# Vectorizer.
vectorizer = TfidfVectorizer(analyzer = 'word', 
                             input = 'content',
                             max_features = 20000,
                             token_pattern = '\w+',
                             ngram_range = (1,1))

# Linear SVC.
def StatisticalModel(train, test):

  train_x, validation_x, train_y, validation_y = train_test_split(train['Preprocessed Text'], train['target'], test_size = 0.2, shuffle=True)

  trainDocs = train_x.tolist()
  trainLabels = train_y.tolist()
  
  validationDocs = validation_x.tolist()
  validationLabels = validation_y.tolist()

  # Training set.
  # Use the same vectorizer to transform the validation and test set.
  train_x = vectorizer.fit_transform(trainDocs)
  train_y = np.asarray(trainLabels)

  # Validation set.
  # Use the trained vectorizer to transform the validation set.
  validation_x = vectorizer.transform(validationDocs)
  validation_y = np.asarray(validationLabels)

  # Fitting the model.
  SVC.fit(train_x, train_y)
  MNB.fit(train_x, train_y)
  LR.fit(train_x, train_y)

  # Get the predictions.
  ## Support Vector Classifier
  SVC_validation_predictions = SVC.predict(validation_x)
  print('The accuracy score for the SVC model is {}'.format(round(accuracy_score(y_true = validation_y,
                                                                                         y_pred = SVC_validation_predictions),
                                                                          3)))
  print('The f1 score for the SVC model is {}'.format(round(f1_score(y_true = validation_y,
                                                                                         y_pred = SVC_validation_predictions),
                                                                    3)))
  print(confusion_matrix(y_true = validation_y, y_pred = SVC_validation_predictions), '\n')


  ## Naive Bayes
  MNB_validation_predictions = MNB.predict(validation_x)
  print('The accuracy score for the Naive Bayes model is {}'.format(round(accuracy_score(y_true = validation_y,
                                                                                         y_pred = MNB_validation_predictions),
                                                                          3)))
  print('The f1 score for the Naive Bayes model is {}'.format(round(f1_score(y_true = validation_y,
                                                                                         y_pred = MNB_validation_predictions),
                                                                    3)))
  print(confusion_matrix(y_true = validation_y, y_pred = MNB_validation_predictions), '\n')

  ## Logistic Regression
  LR_validation_predictions = LR.predict(validation_x)
  print('The accuracy score for the Logistic Regression model is {}'.format(round(accuracy_score(y_true = validation_y,
                                                                                         y_pred = LR_validation_predictions),
                                                                          3)))
  print('The f1 score for the Logistic Regression model is {}'.format(round(f1_score(y_true = validation_y,
                                                                                         y_pred = LR_validation_predictions),
                                                                    3)))
  print(confusion_matrix(y_true = validation_y, y_pred = LR_validation_predictions), '\n')


  ## Test predictions.
  testDocs = test['Preprocessed Text'].tolist()

  # Use the trained vectorizer to transform the test set.
  test_x = vectorizer.transform(testDocs)

  # Get the prediction
  SVC_test_predictions = SVC.predict(test_x)
  MNB_test_predictions = MNB.predict(test_x)
  LR_test_predictions = LR.predict(test_x)

  return SVC_test_predictions, MNB_test_predictions, LR_test_predictions

"""# Training the models.

## Base Text.
"""

SVC_test_predictions, MNB_test_predictions, LR_test_predictions = StatisticalModel(training, test)

SVC_test_predictions, MNB_test_predictions, LR_test_predictions

test['SVC'] = SVC_test_predictions
test['MNB'] = MNB_test_predictions
test['LR'] = LR_test_predictions

test.sample(10)

preds = [SVC_test_predictions, MNB_test_predictions, LR_test_predictions]
p = np.average(preds, axis=0)
final = [round(n) for n in p]
test['final'] = final

test.loc[2665]

test.sample(20)

Submission1 = test[['id']]
Submission2 = test[['id']]
Submission3 = test[['id']]
Submission4 = test[['id']]

Submission1['target'] = SVC_test_predictions
Submission2['target'] = MNB_test_predictions
Submission3['target'] = LR_test_predictions
Submission4['target'] = final

Submission1.to_csv('/gdrive/MyDrive/Kaggle Disaster Tweets/submission1.csv', index=False)
Submission2.to_csv('/gdrive/MyDrive/Kaggle Disaster Tweets/Submission2.csv', index=False)
Submission3.to_csv('/gdrive/MyDrive/Kaggle Disaster Tweets/Submission3.csv', index=False)
Submission4.to_csv('/gdrive/MyDrive/Kaggle Disaster Tweets/Submission4.csv', index=False)