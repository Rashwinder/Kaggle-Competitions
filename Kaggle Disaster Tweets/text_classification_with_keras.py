# -*- coding: utf-8 -*-
"""Text Classification with Keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jU12xaP1rBcGrAWUaTTe2oagQ6p9l6WL

# Libraries.
"""

# Essentials
import pandas as pd
import numpy as np

# Preprocessing
import regex as re
import nltk
nltk.download('punkt')
nltk.download('omw-1.4')
nltk.download('wordnet')
nltk.download("stopwords") # Stopwords.

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
wnl = WordNetLemmatizer()
wst = nltk.tokenize.WhitespaceTokenizer()

# Model.
import tensorflow as tf
import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Embedding, Bidirectional, LSTM
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences
from sklearn.model_selection import train_test_split

import matplotlib.pyplot as plt
import seaborn as sns

"""# Shifting to GPU on Colab."""

tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))

"""## Checking if GPU is enabled."""

from keras import backend as K
K._get_available_gpus()

# Commented out IPython magic to ensure Python compatibility.
# Mounting Google Drive onto Colab.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive

# Reproducibility.
import random
random.seed(1234)

"""# Reading the data set."""

# Data sets.
training = pd.read_csv('/gdrive/MyDrive/Kaggle Competitions/NLP/Kaggle Disaster Tweets/train.csv')
test = pd.read_csv('/gdrive/MyDrive/Kaggle Competitions/NLP/Kaggle Disaster Tweets/test.csv')

# Stopwords.
en_stop = set(stopwords.words('english'))

# Checking the shape of the data frames.
training.shape, test.shape, len(en_stop)

"""# Data PreProcessing."""

import string
def cleaning(x):
  # Lower case, then stripping leading and ending spaces.
  x = x.lower().strip()

  # Retweets.
  x = re.sub(r"RT[\s]+", '', x)

  # Websites.
  x = re.sub(r"https?://[^\s\n\r]+", '', x)

  # Punctuations
  x = re.sub(r'[^a-zA-Z0-9]', ' ', x)

  # Lemmatization.
  x = ' '.join([wnl.lemmatize(w) for w in wst.tokenize(x)])
  return x

# Cleaning the training text.
training['Preprocessed Text'] = training['text'].apply(lambda x: cleaning(x))

# Cleaning the testing text.
test['Preprocessed Text'] = test['text'].apply(lambda x: cleaning(x))

training.sample(10)

"""## Sequencing.
In order to use the keras model, we have to convert our text to sequence data. This can be done through keras's preprocessing module.
"""

tokenizer = Tokenizer()
tokenizer.fit_on_texts(training['Preprocessed Text'])
index = tokenizer.word_index
training_sequence = tokenizer.texts_to_sequences(training['Preprocessed Text'])

"""Checking if it worked."""

training['Preprocessed Text'].loc[0].split(' ')[0], index['our'], training_sequence[0][0]

"""This ensures the sequences all of the same length by either padding, or truncating the sequences. The maximum length is set to 100."""

max_length = max([len(s.split()) for s in training['Preprocessed Text']])
padded_train = pad_sequences(training_sequence,
                             padding = "post",
                             truncating="post",
                             maxlen = max_length)

"""# Generating the train/test split."""

train_X, test_X, train_y, test_y = train_test_split(padded_train, training['target'], test_size = 0.2)
train_X.shape, train_y.shape

train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size = 0.1)
train_X.shape, val_X.shape, test_X.shape, train_y.shape, val_y.shape, test_y.shape

"""# Model Building.
This Keras model uses 3 Bidirectional LSTMs, 1 ReLu Dense layer and a Sigmoid Dense layer.

The embedding layer converts positive integers into dense vectors of a fixed size. In this case, we know that the max length = 34. Each layer is accompanied by a dropout layer which helps to prevent overfitting.

The purpose of the bidirectional LSTMs are to allow the neural network model to read the sequences in both directions in order to preserve the past and future information stored in the embeddings.
"""

model = Sequential()

model.add(Embedding(15000, output_dim = 64, input_length = max_length))

model.add(Bidirectional(LSTM(32,return_sequences=True)))
model.add(Dropout(0.01))

model.add(Bidirectional(LSTM(32,return_sequences=True)))
model.add(Dropout(0.01))

model.add(Bidirectional(LSTM(32,return_sequences=True)))
model.add(Dropout(0.01))

model.add(Dense(8,activation="relu"))
model.add(Dropout(0.01))

model.add(Dense(1,activation="sigmoid"))

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

history = model.fit(train_X, train_y, validation_data=[val_X, val_y],
                    epochs = 7,
                    batch_size = 64)

"""# Visualisation."""

fig, ax = plt.subplots(1, 2)

ax[0].plot(history.history['accuracy'])
ax[0].plot(history.history['val_accuracy'])
ax[0].set_title('Accuracy')
ax[0].set_xlabel('Epochs')
ax[0].legend(['Train', 'Validation'], loc='upper left')

ax[1].plot(history.history['loss'])
ax[1].plot(history.history['val_loss'])
ax[1].set_title('Loss')
ax[1].set_xlabel('Epochs')
ax[1].legend(['Train', 'Validation'], loc='upper left')

"""# Submission.

## Converting the testing text to sequences.
"""

test_sequence = tokenizer.texts_to_sequences(test['Preprocessed Text'])

"""## Limiting the length of the sequences."""

padded_test = pad_sequences(test_sequence,
                             padding = "post",
                             truncating = "post",
                             maxlen = max_length)

"""## Predicting."""

test_predictions = model.predict(padded_test)
test_predictions = [1 if each[0] >= 0.5 else 0 for each in test_predictions]
test['target'] = test_predictions

"""## Submission."""

Submission = test[['id', 'target']]
Submission.to_csv('/gdrive/MyDrive/Kaggle Competitions/NLP/Kaggle Disaster Tweets/Keras_attempt.csv', index = False)